{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyONI7GdZ9Pb8P7xtPn1SHvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adhvik09-oss/Adhvik09-oss/blob/main/SnapCareipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6-N9m6rmOrUw"
      },
      "outputs": [],
      "source": [
        "#importing out neccessary modules\n",
        "import torch as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#writing our device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "GrEhK0QxWO1s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for more information on the different transfer learning architectures look at:\n",
        "https://docs.pytorch.org/vision/0.9/models.html"
      ],
      "metadata": {
        "id": "rgiVIptlWp6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet50(pretrained=True)"
      ],
      "metadata": {
        "id": "NsOq_E_mWJML"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "unLuajENWlTL",
        "outputId": "c8d45452-3a63-4740-bf52-da54c4b1cf78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# we have to change the final layer because our dataset(SD-198) has 198 classes, unlike the standard 100 that resnet-50 comes with\n",
        "resnet.fc = nn.Linear(in_features=2048, out_features=198)"
      ],
      "metadata": {
        "id": "adWx2uQuWSu9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze all the layers so we won't have to train it fully\n",
        "for param in resnet.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "#unfreeze the fully connected layer\n",
        "for param in resnet.fc.parameters():\n",
        "  param.requires_grad = True"
      ],
      "metadata": {
        "id": "8fBZeT8RZs1_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting our loss function and optimizers\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(params = resnet.parameters(),\n",
        "                            lr = 0.01,\n",
        "                            momentum = 0.09)"
      ],
      "metadata": {
        "id": "MJiy1BkXa0fV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing our dataset and preparing to create a dataloader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUgdShdkhE7t",
        "outputId": "58753ff1-2b38-493c-9636-b14e920a887e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "skin_data_path = '/content/drive/MyDrive/data.zip' #this is the google drive path of the dataset\n",
        "extract_path = '/content/SD-198'  # Local Colab path to unzip\n",
        "\n",
        "with zipfile.ZipFile(skin_data_path, 'r') as zip_ref: #this simply just extracts the zip file with data to a certain path\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "N9ziPAgimdqc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#double checking the strucure of our data\n",
        "\n",
        "print(os.listdir(extract_path))  # Should list class folders\n",
        "sample_class = os.listdir(extract_path)[0]\n",
        "print(\"Sample images in class:\", sample_class)\n",
        "print(os.listdir(os.path.join(extract_path, sample_class))[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SumadNluDOj",
        "outputId": "1c70d6f6-1faa-41ec-9d6c-47d91ed42ebe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Racquet_Nail', 'Scarring_Alopecia', 'Drug_Eruption', 'Cellulitis', 'Basal_Cell_Carcinoma', 'Fibroma', 'Tinea_Corporis', 'Nail_Dystrophy', 'Androgenetic_Alopecia', 'Eccrine_Poroma', 'Granuloma_Annulare', 'Nevus_Spilus', 'Halo_Nevus', 'Pyoderma_Gangrenosum', 'Beckers_Nevus', 'Onychoschizia', 'Blue_Nevus', 'Discoid_Lupus_Erythematosus', 'Actinic_solar_Damage(Actinic_Cheilitis)', 'Pityriasis_Alba', 'Darier-White_Disease', 'Tinea_Faciale', 'Seborrheic_Keratosis', 'Geographic_Tongue', 'Onychomycosis', 'Beaus_Lines', 'Follicular_Retention_Cyst', 'Psoriasis', 'Xerosis', 'Actinic_solar_Damage(Actinic_Keratosis)', 'Stasis_Dermatitis', 'Crowes_Sign', 'Callus', 'Acne_Vulgaris', 'Paronychia', 'Cutanea_Larva_Migrans', 'Eczema', 'Hidradenitis_Suppurativa', 'Impetigo', 'Dyshidrosiform_Eczema', 'Mal_Perforans', 'Keratoacanthoma', 'Terrys_Nails', 'Ichthyosis', 'Actinic_solar_Damage(Solar_Purpura)', 'Lymphangioma_Circumscriptum', 'Ganglion', 'Erythema_Ab_Igne', 'Neurodermatitis', 'Junction_Nevus', 'Follicular_Mucinosis', 'Actinic_solar_Damage(Pigmentation)', 'Trichilemmal_Cyst', 'Mucous_Membrane_Psoriasis', 'Actinic_solar_Damage(Cutis_Rhomboidalis_Nuchae)', 'Actinic_solar_Damage(Telangiectasia)', 'Leukocytoclastic_Vasculitis', 'Neurofibroma', 'Nevus_Comedonicus', 'Arsenical_Keratosis', 'Melasma', 'Dermatofibroma', 'Alopecia_Areata', 'Tinea_Manus', 'Keratolysis_Exfoliativa_of_Wende', 'Rosacea', 'Striae', 'Hyperkeratosis_Palmaris_Et_Plantaris', 'Linear_Epidermal_Nevus', 'Solar_Lentigo', 'Perioral_Dermatitis', 'Factitial_Dermatitis', 'Leukonychia', 'Bowenoid_Papulosis', 'Kerion', 'Steroid_Striae', 'Pseudofolliculitis_Barbae', 'Allergic_Contact_Dermatitis', 'Cafe_Au_Lait_Macule', 'Onychogryphosis', 'Morphea', 'Pitted_Keratolysis', 'Epithelioma_Adenoides_Cysticum', 'Pityriasis_Rosea', 'Exfoliative_Erythroderma', 'Histiocytosis_X', 'Granulation_Tissue', 'Livedo_Reticularis', 'Disseminated_Actinic_Porokeratosis', 'Dilated_Pore_of_Winer', 'Cutaneous_T-Cell_Lymphoma', 'Actinic_solar_Damage(Solar_Elastosis)', 'Fixed_Drug_Eruption', 'Hypertrichosis', 'Lichen_Spinulosis', 'Steroid_Use_abusemisuse_Dermatitis', 'Herpes_Zoster', 'Cutaneous_Horn', 'Balanitis_Xerotica_Obliterans', 'Sebaceous_Gland_Hyperplasia', 'Skin_Tag', 'Molluscum_Contagiosum', 'Leiomyoma', 'Nevus_Sebaceous_of_Jadassohn', 'Epidermal_Nevus', 'Lymphocytic_Infiltrate_of_Jessner', 'Trichofolliculoma', 'Chalazion', 'Fibroma_Molle', 'Erythema_Craquele', 'Nail_Ridging', 'Fordyce_Spots', 'Apocrine_Hydrocystoma', 'Compound_Nevus', 'Tinea_Versicolor', 'Milia', 'Stomatitis', 'Clubbing_of_Fingers', 'Radiodermatitis', 'Pomade_Acne', 'Lentigo_Maligna_Melanoma', 'Frictional_Lichenoid_Dermatitis', 'Herpes_Simplex_Virus', 'Pearl_Penile_Papules', 'Lipoma', 'Inverse_Psoriasis', 'Stasis_Edema', 'Stasis_Ulcer', 'Acute_Eczema', 'Schambergs_Disease', 'Tinea_Pedis', 'Pustular_Psoriasis', 'Syringoma', 'Benign_Keratosis', 'Angioma', 'Epidermoid_Cyst', 'Kyrles_Disease', 'Toe_Deformity', 'Scalp_Psoriasis', 'Desquamation', 'Cutis_Marmorata', 'Nevus_Incipiens', 'Subungual_Hematoma', 'Aphthous_Ulcer', 'Candidiasis', 'Nail_Psoriasis', 'Lichen_Planus', 'Strawberry_Hemangioma', 'Lichen_Sclerosis_Et_Atrophicus', 'Favre_Racouchot', 'Acrokeratosis_Verruciformis', 'Dry_Skin_Eczema', 'Neurotic_Excoriations', 'Digital_Fibroma', 'Green_Nail', 'Wound_Infection', 'Urticaria', 'Hailey_Hailey_Disease', 'Dysplastic_Nevus', 'Verruca_Vulgaris', 'Vitiligo', 'Malignant_Melanoma', 'Dermatosis_Papulosa_Nigra', 'Varicella', 'Trichostasis_Spinulosa', 'Pseudorhinophyma', 'Nummular_Eczema', 'Rhinophyma', 'Scar', 'Ulcer', 'Guttate_Psoriasis', 'Metastatic_Carcinoma', 'Seborrheic_Dermatitis', 'Koilonychia', 'Acne_Keloidalis_Nuchae', 'Keratosis_Pilaris', 'Erythema_Multiforme', 'Keloid', 'Nail_Nevus', 'Bowens_Disease', 'Erythema_Annulare_Centrifigum', 'Pincer_Nail_Syndrome', 'Poikiloderma_Atrophicans_Vasculare', 'Lymphomatoid_Papulosis', 'Pityrosporum_Folliculitis', 'Myxoid_Cyst', 'Pyogenic_Granuloma', 'Congenital_Nevus', 'Angular_Cheilitis', 'Median_Nail_Dystrophy', 'Steroid_Acne', 'Infantile_Atopic_Dermatitis', 'Behcets_Syndrome', 'Mucha_Habermann_Disease', 'Lichen_Simplex_Chronicus', 'Onycholysis', 'Half_and_Half_Nail', 'Tinea_Cruris']\n",
            "Sample images in class: Racquet_Nail\n",
            "['5113.jpg', '5117.jpg', '5118.jpg', '5109.jpg', '5108.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split # import random split for later use\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_data = transforms.Compose([\n",
        "    transforms.Resize((224,224)), #resizes img to 224x224 which is what resnet uses\n",
        "    transforms.ToTensor(), #transforms from PIL to Pytorch Tensor\n",
        "    transforms.Normalize([0.485,0.456,0.406],\n",
        "                          [0.229, 0.224, 0.225])\n",
        "\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root = extract_path, transform = train_data) # this is our data in the form of a Image Folder, we will have to change this into a\n",
        "\n",
        "train_split = int(0.8*len(dataset))\n",
        "testing_split = len(dataset)-train_split\n",
        "\n",
        "train_dataset,test_dataset = random_split(dataset,[train_split,testing_split])\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size = 32, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset,batch_size = 32, shuffle = False)"
      ],
      "metadata": {
        "id": "5FCJexcInSG5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using our premade accuracy_function\n",
        "def accuracy(y_true,y_pred):\n",
        "  correct = torch.eq(y_true,y_pred).sum().item()#gives amount of correct preds in the form of a python int\n",
        "  acc = (correct/len(y_pred))*100#divdes it by total num of predictions to return a accuracy\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "Lz0nwSUgtO9W"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing a function previously made to measure the time\n",
        "def print_train_time(start:float,\n",
        "                     end:float,\n",
        "                     device: torch.device = None):\n",
        "\n",
        "  \"print difference between start and end time\"\n",
        "  total_time = end-start\n",
        "  print(f\"train time on {device}: {total_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "Q92heveS-PNI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing our previously made model evalutaion function:\n",
        "from tqdm.auto import tqdm\n",
        "def eval_model(model:torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "  '''returns a dictionary contatining hte results of model predicting on data_loader'''\n",
        "  loss,acc = 0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in tqdm(data_loader):\n",
        "      #make predictions\n",
        "      y_pred = model(X)\n",
        "      #Accumulate the loss and the acc values per batch\n",
        "      loss+= loss_fn(y_pred,y)\n",
        "      acc+= accuracy_fn(y_true=y,\n",
        "                        y_pred = y_pred.argmax(dim=1)) #raw ouputs are logits accuracy only takes true labels so we need to label it\n",
        "    #Scale loss and acc to find avg loss/acc per batch\n",
        "    loss/=len(data_loader)\n",
        "    acc/= len(data_loader)\n",
        "    return {\n",
        "            \"model_loss\":loss.item(),\n",
        "            \"model_acc\":acc}\n"
      ],
      "metadata": {
        "id": "9jEs2q3M-7cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing our previously made training and testing loop functions:\n",
        "def train_step(model: torch.nn.Module(),\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module(),\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy,\n",
        "               device: torch.device = device):\n",
        "  pass\n",
        "  train_loss,train_acc = 0,0\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    #do the forward pass\n",
        "    y_pred = model(X)\n",
        "    #calculate loss\n",
        "    loss = loss_fn(y_pred,y)\n",
        "    #accumulate loss\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "    optimizer.zero_grad()\n",
        "    #loss backward\n",
        "    loss.backward()\n",
        "    #optimizer zero\n",
        "    optimizer.step()\n",
        "    #print what's happening\n",
        "\n",
        "  #divide total training loss over len\n",
        "  train_loss /= len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(model: torch.nn.Module(),\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module(),\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              accuracy,\n",
        "              device: torch.device = device):\n",
        "  test_acc,test_loss = 0,0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for X,y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      #forward pass\n",
        "      test_pred = model(X) #raw logits\n",
        "\n",
        "    # 2. Calculate loss and accuracy\n",
        "      test_loss += loss_fn(test_pred, y)\n",
        "      test_acc += accuracy(y_true=y,\n",
        "                  y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "              )\n",
        "    test_loss/=len(dataloader)\n",
        "    test_acc/=len(dataloader)\n",
        "    print(f\"\\n train loss: {train_loss} Test loss {test_loss}, test_acc {test_acc}\")"
      ],
      "metadata": {
        "id": "5ljj4IkF-xe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGq5bHxR_ti7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}